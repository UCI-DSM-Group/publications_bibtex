@inproceedings{2010-middleware,
author = {Jalali, Leila and Mehrotra, Sharad and Venkatasubramanian, Nalini},
title = {Middleware Solutions for Integrated Simulation Environments},
year = {2010},
isbn = {9781450304573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1891748.1891749},
doi = {10.1145/1891748.1891749},
abstract = {This paper outlines the expected research contribution of my PhD work. RAISE is a research project aimed at building a framework and platform that supports the integration of multiple existing models, simulations, and data. One of the limitations of the simulators is that they are developed by domain experts who have an in-depth understanding of the phenomena being modelled and typically designed to be executed and evaluated independently. Therefore the grand challenge is to facilitate the process of pulling all of independently created models together into an interoperating system of systems model where decision makers can explore different alternatives and conduct low cost experiments in an interactive environment. The key research question is whether such integration of independently created, deep domain models can be made feasible, practical, flexible, cost-effective, attractive, and usable.},
booktitle = {Proceedings of the 7th Middleware Doctoral Symposium},
pages = {2–7},
numpages = {6},
keywords = {metamodel, simulation integration, structural reflection, reflective middleware},
location = {Bangalore, India},
series = {MDS '10}
}


@inproceedings{2010-farecast,
abstract = {To disseminate messages from a single source to a large number of targeted receivers, a natural approach is the tree-based application layer multicast (ALM). However, in time-constrained flash dissemination scenarios, e.g. earthquake early warning, where time is of the essence, the tree-based ALM has a single point of failure; its reliable extensions using ack-based failure recovery protocols cannot support reliable dissemination in the timeframe needed. In this paper, we exploit path diversity, i.e. exploit the use of multiple data paths, to achieve fast and reliable data dissemination. First, we design a forest-based M2M (Multiple parents-To-Multiple children) ALM structure where every node has multiple children and multiple parents. The intuition is to enable lower dissemination latency through multiple children, while enabling higher reliability through multiple parents. Second, we design multidirectional multicasting algorithms that effectively utilize the multiple data paths in the M2M ALM structure. A key aspect of our reliable dissemination mechanism is that nodes, in addition to communicating the data to children, also selectively disseminate the data to parents and siblings. As compared to trees using traditional multicasting algorithm, we observe an 80{\{}{\%}{\}} improvement in reliability under 20{\{}{\%}{\}} of failed nodes with no significant increase in latency for over 99{\{}{\%}{\}} of the nodes.},
address = {Berlin, Heidelberg},
author = {Kim, Kyungbaek and Mehrotra, Sharad and Venkatasubramanian, Nalini},
booktitle = {Middleware 2010},
editor = {Gupta, Indranil and Mascolo, Cecilia},
isbn = {978-3-642-16955-7},
pages = {169--190},
publisher = {Springer Berlin Heidelberg},
title = {{FaReCast: Fast, Reliable Application Layer Multicast for Flash Dissemination}},
year = {2010}
}

@INPROCEEDINGS{2010-assessing,  
author={K. {Kim} and N. {Venkatasubramanian}},  
booktitle={2010 IEEE Global Telecommunications Conference GLOBECOM 2010},   
title={Assessing the Impact of Geographically Correlated Failures on Overlay-Based Data Dissemination},   
year={2010},  
volume={},  
number={},  
pages={1-5},  
abstract={This paper addresses reliability of data dissemination applications when there are severe disruptions to the underlying physical infrastructure. Such massive simultaneous physical failures can happen during the geographical events such as natural disasters (earthquakes, floods, tornados) or sudden power outages - infrastructure failures in these cases are geographically correlated. In particular, we focus on overlay based data dissemination mechanisms and explore their ability to tolerate such large geographically correlated failures. Due to the tight correlation between multiple overlay links and a single physical link, a few physical failures may affect lots of overlay links. To enable reliable dissemination under such conditions, we propose overlay network construction methods that incorporate proximity-aware neighbor selection methods to improve the performance of the overlay data dissemination, to the extent possible, in terms of reliability and latency. In this approach, the overlay nodes select neighbors which are most likely distinct in presence of a geographical failure; we show how an overlay structure constructed using our proximity-aware neighbor selection techniques can disseminate data to over 80% of reachable end clients without any significant additional latency under various geographical failure conditions.},  keywords={computer network reliability;information dissemination;telecommunication network routing;telecommunication network topology;geographical correlated failures;overlay-based data dissemination;physical infrastructure failures;natural disasters;overlay links;overlay network construction methods;proximity-aware neighbor selection methods;Peer to peer computing;Reliability;Routing;Correlation;Topology;Government;IEEE Communications Society},  
doi={10.1109/GLOCOM.2010.5685229},  
ISSN={1930-529X},  
month={Dec},
}

@ARTICLE{2010-semantic,  
author={D. {Kalashnikov} and S. {Mehrotra} and J. {Xu} and N. {Venkatasubramanian}},  
journal={IEEE Transactions on Knowledge and Data Engineering},   
title={A Semantics-Based Approach for Speech Annotation of Images},   
year={2011},  
volume={23},  
number={9},  
pages={1373-1387},  
abstract={Associating textual annotations/tags with multimedia content is among the most effective approaches to organize and to support search over digital images and multimedia databases. Despite advances in multimedia analysis, effective tagging remains largely a manual process wherein users add descriptive tags by hand, usually when uploading or browsing the collection, much after the pictures have been taken. This approach, however, is not convenient in all situations or for many applications, e.g., when users would like to publish and share pictures with others in real time. An alternate approach is to instead utilize a speech interface using which users may specify image tags that can be transcribed into textual annotations by employing automated speech recognizers. Such a speech-based approach has all the benefits of human tagging without the cumbersomeness and impracticality typically associated with human tagging in real time. The key challenge in such an approach is the potential low recognition quality of the state-of-the-art recognizers, especially, in noisy environments. In this paper, we explore how semantic knowledge in the form of co-occurrence between image tags can be exploited to boost the quality of speech recognition. We postulate the problem of speech annotation as that of disambiguating among multiple alternatives offered by the recognizer. An empirical evaluation has been conducted over both real speech recognizer's output as well as synthetic data sets. The results demonstrate significant advantages of the proposed approach compared to the recognizer's output under varying conditions.},  keywords={multimedia databases;speech recognition;semantics based approach;speech annotation;multimedia database;speech based approach;speech recognition;multimedia analysis;Speech;Speech recognition;Semantics;Tagging;Correlation;Real time systems;Image recognition;Using speech for tagging and annotation;using semantics to improve ASR;maximum entropy approach;correlation-based approach;branch and bound algorithm.},  
doi={10.1109/TKDE.2010.185},  
ISSN={1558-2191},  
month={Sep.},
}


@article{2010-partitioning,
author = {Lee, Kyoungwoo and Shrivastava, Aviral and Dutt, Nikil and Venkatasubramanian, Nalini},
title = {Partitioning Techniques for Partially Protected Caches in Resource-Constrained Embedded Systems},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1084-4309},
url = {https://doi.org/10.1145/1835420.1835423},
doi = {10.1145/1835420.1835423},
abstract = {Increasing exponentially with technology scaling, the soft error rate even in earth-bound embedded systems manufactured in deep subnanometer technology is projected to become a serious design consideration. Partially protected cache (PPC) is a promising microarchitectural feature to mitigate failures due to soft errors in power, performance, and cost sensitive embedded processors. A processor with PPC maintains two caches, one protected and the other unprotected, both at the same level of memory hierarchy. The intuition behind PPCs is that not all data in the application is equally prone to soft errors. By finding and mapping the data that is more prone to soft errors to the protected cache, and error-resilient data to the unprotected cache, failures induced by soft errors can be significantly reduced at a minimal power and performance penalty. Consequently, the effectiveness of PPCs critically hinges on the compiler's ability to partition application data into error-prone and error-resilient data. The effectiveness of PPCs has previously been demonstrated on multimedia applications—where an obvious partitioning of data exists, the multimedia data is inherently resilient to soft errors, and the rest of the data and the entire code is assumed to be error-prone. Since the amount of multimedia data is a quite significant component of the entire application data, this obvious partitioning is quite effective. However, no such obvious data and code partitioning exists for general applications. This severely restricts the applicability of PPCs to data caches and instruction caches in general. This article investigates vulnerability-based partitioning schemes that are applicable to applications in general and effectively reduce failures due to soft errors at minimal power and performance overheads.Our experimental results on an HP iPAQ-like processor enhanced with PPC architecture, running benchmarks from the MiBench suite demonstrate that our partitioning heuristic efficiently finds page partitions for data PPCs that can reduce the failure rate by 48% at only 2% performance and 7% energy overhead, and finds page partitions for instruction PPCs that reduce the failure rate by 50% at only 2% performance and 8% energy overhead, on average.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = oct,
articleno = {30},
numpages = {30},
keywords = {Page partitioning technique, partially protected cache, embedded systems, soft error, vulnerability}
}


@INPROCEEDINGS{2010-tolerant,  
author={B. {Xing} and S. {Mehrotra} and N. {Venkatasubramanian}},  
booktitle={2010 7th Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks (SECON)},   
title={Disruption-Tolerant Spatial Dissemination},   
year={2010},  
volume={},  
number={},  
pages={1-9},  
abstract={Spatial dissemination is a specific form of information dissemination that enables mobile users to send information to other mobile users who are or will appear at a specific location (a user-defined region). Such geo-messaging services are on the rise; they typically are built upon centralized solutions and require users to have reliable access to a stable backend infrastructure for storing and communicating content. In this paper, we develop a distributed solution to spatial dissemination, that can work without the need for such an infrastructure. Our solution utilizes the concepts from disruption-tolerant networking to build a flexible/best-effort service that leverages the intermittent ad-hoc connectivity between users. We propose Sticker, a spatial dissemination protocol that aims to maximize delivery reliability without incurring significant storage/transmission overheads. Sticker employs the store-carry-and- forward model, and strives to optimize dissemination performance by addressing three sub-problems - replication, forwarding and purging. Our experiments show that, Sticker achieves delivery ratios that are close to the maximum possible values; as compared to existing techniques, it either cuts down storage/transmission overheads by over 50%, or greatly enhances both delivery reliability and storage efficiency.},  keywords={fault tolerant computing;information dissemination;mobile computing;protocols;information dissemination;mobile users;geomessaging services;disruption tolerant network;Sticker protocol;spatial dissemination protocol;store-carry-and-forward model;mobile computing;Web server;Mobile computing;Telecommunication network reliability;Disruption tolerant networking;Databases;Web and internet services;Mobile communication;Communications Society;Computer science;Protocols},  
doi={10.1109/SECON.2010.5508274},  
ISSN={2155-5494},  
month={June},
}

@inproceedings{2010-planning,
author = {Balasubramanian, Vidhya and Kalashnikov, Dmitri V. and Mehrotra, Sharad and Venkatasubramanian, Nalini},
title = {Efficient and Scalable Multi-Geography Route Planning},
year = {2010},
isbn = {9781605589459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1739041.1739090},
doi = {10.1145/1739041.1739090},
abstract = {This paper considers the problem of Multi-Geography Route Planning (MGRP) where the geographical information may be spread over multiple heterogeneous interconnected maps. We first design a flexible and scalable representation to model individual geographies and their interconnections. Given such a representation, we develop an algorithm that exploits precomputation and caching of geographical data for path planning. A utility-based approach is adopted to decide which paths to precompute and store. To validate the proposed approach we test the algorithm over the workload of a campus level evacuation simulation that plans evacuation routes over multiple geographies: indoor CAD maps, outdoor maps, pedestrian and transportation networks, etc. The empirical results indicate that the MGRP algorithm with the proposed utility based caching strategy significantly outperforms the state of the art solutions when applied to a large university campus data under varying conditions.},
booktitle = {Proceedings of the 13th International Conference on Extending Database Technology},
pages = {394–405},
numpages = {12},
location = {Lausanne, Switzerland},
series = {EDBT '10}
}


@INPROCEEDINGS{2010-gateway,  
author={B. {Xing} and M. {Deshpande} and S. {Mehrotra} and N. {Venkatasubramanian}},  
booktitle={2010 8th IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)},   
title={Gateway designation for timely communications in instant mesh networks},   
year={2010},  
volume={},  
number={},  
pages={564-569},  
abstract={In this paper, we explore how to effectively create and use ¿instant mesh networks¿, i.e., wireless mesh networks that are dynamically deployed in temporary circumstances (e.g., emergency responses) - in addition to enabling coverage for internal on-site communications, such a network will support information flow into and out of the deployment site through its gateway (i.e., the mesh router that connects to the external backhaul). We study optimizing the performance of communications (specifically in terms of latency) in an instant mesh network by intelligently selecting the gateway. We demonstrate that designating the proper gateway significantly enhances the timeliness of communications with the external backhaul. We mathematically model the ¿gateway designation problem¿ using the notion of centrality from graph theory. We propose a distributed algorithm, FACE (Fast Approximate Center Exploration), for locating the optimal gateway. FACE is an approximate algorithm that works in an efficient manner without compromising the optimality of solutions. A thorough performance evaluation shows that the gateways designated by FACE reduce latencies by up to 92% for various types of communications, and that FACE saves transmission cost and execution time by up to 71% in finding the gateways.},  keywords={graph theory;wireless mesh networks;gateway designation;timely communications;instant mesh networks;wireless mesh networks;graph theory;fast approximate center exploration;FACE;Mesh networks;Delay;Costs;Spine;Telecommunication network reliability;Wireless mesh networks;Image sensors;Multimodal sensors;Speech analysis;Spread spectrum communication;mesh network;gateway;centrality;approximation},  
doi={10.1109/PERCOMW.2010.5470501},  
ISSN={},  
month={March},
}



@INPROCEEDINGS{2010-localization,  
author={S. {Bonetti} and S. {Mehrotra} and N. {Venkatasubramanian}},  
booktitle={2010 8th IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)},   
title={Exploring quality in multisensor pervasive systems - a localization case study},   
year={2010},  
volume={},  
number={},  
pages={81-86},  
abstract={This paper addresses quality vs. cost tradeoffs in multisensor pervasive spaces. Specifically, we focus on a case study that uses location sensing in instrumented pervasive spaces executing a variety of applications ranging from social networking to surveillance and security. Location sensing is vital in creating effective situational awareness for mission critical applications, e.g. localization of firefighters and rescue personnel in emergency response scenarios as well as general purpose location based services. While advances in GPS and other technologies have enabled high quality outdoor localization, the realization of accurate indoor location technologies is significantly more complex. We present a generalized indoor localization framework that composes inputs from multiple localization technologies in a quality-aware manner so as to meet the accuracy/cost demands of diverse applications.},  keywords={mobile computing;sensor fusion;multisensor pervasive systems;social networking;location sensing;situational awareness;mission critical application;generalized indoor localization framework;multiple localization technology;Space technology;Costs;Instruments;Monitoring;Humans;Gas detectors;Acoustic sensors;Social network services;Surveillance;Personnel},  
doi={10.1109/PERCOMW.2010.5470607},  
ISSN={},  
month={March},
}

@INPROCEEDINGS{2010-quality,  
author={C. {Davison} and D. {Massaguer} and L. {Paradis} and M. {Reza Rahimi} and  {Bo Xing} and Q. {Han} and S. {Mehrotra} and N. {Venkatasubramanian}},  
booktitle={2010 8th IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)},   
title={Practical experiences in enabling and ensuring quality sensing in emergency response applications},   
year={2010},  
volume={},  
number={},  
pages={388-393},  
abstract={Situational awareness in emergency response is critical. Knowing the status of the hazards, the rescue workers, and the building occupants, etc., can greatly help the incident commander make the right decisions in responding emergencies, and as a result, save lives. Such situational awareness can be achieved by using existing sensing and communication infrastructures and/or having rescue practitioners dynamically deploy their own. Nevertheless, current sensing and communications techniques are, not fault-free. In this paper, we study, through both lab experiments and real emergency response drills, the nature of different wireless networks (sensor networks and Wi-Fi networks) in transmitting various types of data. Based on our findings, we propose a series of practical techniques that potentially enhances the reliability of data delivery over heterogeneous wireless networks, by exploiting the availability of multiple networks, the rescue workers' mobility, and the possibility of having rescue teams carry redundant sensors.},  keywords={emergency services;wireless sensor networks;quality sensing;emergency response applications;situational awareness;incident commander;communication infrastructures;rescue practitioners;communications techniques;sensor networks;Wi-Fi networks;data transmittion;data delivery reliability;heterogeneous wireless networks;rescue teams;redundant sensors;Wireless sensor networks;Network topology;Base stations;Mission critical systems;Buildings;Computer science;Decision making;Performance loss;Telecommunication traffic;Application software},  
doi={10.1109/PERCOMW.2010.5470636},  
ISSN={},  
month={March},
}

