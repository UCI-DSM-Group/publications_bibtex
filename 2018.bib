@INPROCEEDINGS{2018-ride,
  author={K. E. {Benson} and G. {Wang} and N. {Venkatasubramanian} and Y. {Kim}},
  booktitle={2018 IEEE/ACM Third International Conference on Internet-of-Things Design and Implementation (IoTDI)}, 
  title={Ride: A Resilient IoT Data Exchange Middleware Leveraging SDN and Edge Cloud Resources}, 
  year={2018},
  volume={},
  number={},
  pages={72-83},
  abstract={Internet of Things (IoT) deployments rely on data exchange middleware to manage communications between constrained devices and cloud resources that provide analytics, data storage, and serve user applications. In this paper, we propose the Resilient IoT Data Exchange (Ride) middleware that enables resilient operation of IoT applications despite prevalent network failures and congestion. It leverages programmable Software-Defined Networking (SDN)-enabled infrastructure along with both localized edge and cloud services. The two-phase Ride middleware extends existing publish-subscribe oriented IoT data exchanges according to application-specified resilience requirements and without IoT device client modifications. The first phase, Ride-C, improves IoT data collection by gathering network-awareness via a novel resource-aware adaptive probing mechanism and dynamically redirecting IoT data flows across multiple public and local (edge) cloud data exchange connections. The second phase, Ride-D, uses this information to disseminate time-critical alerts via an intelligent network-aware resilient multicast mechanism. Results from our prototype smart campus testbed implementation, Mininet-based emulated experiments, and larger-scale simulations show that Ride enables network awareness for greater cloud connection up-times, timely fail-over to edge services, and more resilient local alert dissemination.},
  keywords={cloud computing;Internet of Things;middleware;software defined networking;edge cloud resources;constrained devices;data storage;user applications;localized edge;cloud services;two-phase Ride middleware;IoT device client modifications;IoT data collection;network-awareness;local cloud data;intelligent network-aware resilient multicast mechanism;network awareness;edge services;resilient local alert dissemination;Internet of Things;network failures;resilient IoT data exchange middleware leveraging SDN;programmable software-defined networking;resource-aware adaptive probing mechanism;cloud connection up-times;IoT data exchanges;Cloud computing;Earthquakes;Reliability;Resilience;Sensors;Protocols;IoT;data exchange;SDN;resilience;multicast;seismic;alerting;pub-sub;edge cloud;fail-over;failure-detection},
  doi={10.1109/IoTDI.2018.00017},
  ISSN={},
  month={April}
  }

@INPROCEEDINGS{2018-infocom,
  title     = "Spatiotemporal Scheduling for Crowd Augmented Urban Sensing",
  booktitle = "{IEEE} {INFOCOM} 2018 - {IEEE} Conference on Computer
               Communications",
  author    = "Zhu, Q and Sarwar Uddin, M Y and Venkatasubramanian, N and Hsu,
               C",
  abstract  = "In urban environments, mobile crowdsensing can be used to
               augment in-situ sensing deployments (e.g. for environmental and
               community monitoring) in a flexible and cost-efficient manner.
               The additional participation provided by crowdsensing enables
               improved data collection coverage and enhances timeliness of
               data delivery. However, as the number of participating
               devices/users increases, efficient management is required to
               handle the increased operational cost of the infrastructure and
               associated cloud services - exploiting spatiotemporal redundancy
               in sensing can help cost-efficient utilization of resources. In
               this paper, we develop solutions to exploit the mobility of the
               crowd and manage the sensing capability of participating devices
               to effectively meet application/user demands for hybrid urban
               sensing applications. Specifically, we address the
               spatiotemporal scheduling problem to create high-resolution maps
               (e.g. for pollution sensing) by developing a common framework to
               capture spatiotemporal impact of multiple sensor types that
               generate heterogeneous data at different levels of granularity.
               We develop an online scheduling approach that leverages the
               knowledge of device location and sensing capability to
               selectively activate nodes and sensors. We build a multi-sensor
               platform that enables data collection, data exchange, and node
               management. Prototype deployments in three different
               campus/community testbeds were instrumented for measurements.
               Traces collected from the testbeds are used to drive extensive
               large scale simulations. Results show that our proposed solution
               achieves improved data coverage and utility under data
               constraints with lower costs (30\% fewer active nodes) than
               naive approaches.",
  pages     = "1997--2005",
  month     =  apr,
  year      =  2018,
  keywords  = "cloud computing;mobile computing;sensors;telecommunication
               scheduling;multisensor platform;data exchange;node
               management;prototype deployments;data coverage;utility;data
               constraints;crowd augmented urban sensing;urban
               environments;mobile crowdsensing;in-situ sensing
               deployments;environmental community monitoring;flexible
               cost-efficient manner;data collection coverage;data
               delivery;participating devices/users increases;efficient
               management;increased operational cost;infrastructure;associated
               cloud services;spatiotemporal redundancy;sensing
               capability;application/user demands;hybrid urban sensing
               applications;spatiotemporal scheduling problem;high-resolution
               maps;pollution sensing;spatiotemporal impact;multiple sensor
               types;heterogeneous data;online scheduling approach;device
               location;selectively activate nodes;sensors;campus/community
               testbeds;Sensors;Spatiotemporal phenomena;Pollution;Processor
               scheduling;Monitoring;Redundancy;Urban areas;DSM"
}

@INPROCEEDINGS{2018-iccps,
  title     = "Impact Driven Sensor Placement for Leak Detection in Community
               Water Networks",
  booktitle = "2018 {ACM/IEEE} 9th International Conference on {Cyber-Physical}
               Systems ({ICCPS})",
  author    = "Venkateswaran, P and Han, Q and Eguchi, R T and
               Venkatasubramanian, N",
  abstract  = "Community water networks have become increasingly prone to
               failures due to aging infrastructure, resulting in an increased
               effort to instrument and monitor networks using IoT (Internet of
               Things) sensors. However, identifying optimal locations to
               instrument these sensors to detect and localize failures such as
               leaks is challenging due to the growing scale and complexity of
               water networks. Current sensor placement algorithms use
               heuristics that focus mainly on enabling network coverage. In
               this paper, we propose a multilevel approach to model and
               quantify the real-world impact of a failure on a community using
               various geospatial, infrastructural and societal factors. We
               present techniques to integrate failure impact, IoT sensing
               data, and simulation based analytics to drive two novel sensor
               placement algorithms with the objective of reducing
               community-scale impact. We evaluate our proposed algorithms on
               various failure scenarios using multiple real-world water
               networks at different scales and compare them to existing
               solutions. The experimental results show that the proposed
               algorithms result in sensor placements that can achieve an 80\%
               reduction in impact while using a comparable number of sensors
               for diverse real-world networks.",
  pages     = "77--87",
  month     =  apr,
  year      =  2018,
  keywords  = "computerised instrumentation;failure analysis;Internet;Internet
               of Things;leak detection;sensor placement;leak
               detection;community water networks;aging
               infrastructure;simulation based analytics;impact driven sensor
               placement algorithms;water network complexity;failure impact
               detection;Internet of Things;IoT sensing data;geospatial
               factors;Instruments;Sociology;Statistics;Water
               resources;Junctions;Leak detection;Monitoring;IoT;CPS;Sensor
               Placement;Water Distribution System;DSM"
}

@MISC{2018-infocom-poster,
  title    = "Poster abstract: Enhancing reliability of community
              {Internet-of-Things} deployments with mobility",
  author   = "Zhu, Qiuxi and Uddin, Md Yusuf Sarwar and Venkatasubramanian,
              Nalini and Hsu, Cheng-Hsin and Hong, Hua-Jun",
  journal  = "IEEE INFOCOM 2018 - IEEE Conference on Computer Communications
              Workshops (INFOCOM WKSHPS)",
  year     =  2018,
  keywords = "DSM"
}

@INPROCEEDINGS{2018-icdcs,
  title     = "Edge Caching for Enriched Notifications Delivery in Big Active
               Data",
  booktitle = "2018 {IEEE} 38th International Conference on Distributed
               Computing Systems ({ICDCS})",
  author    = "Uddin, M Y S and Venkatasubramanian, N",
  abstract  = "In this paper, we propose a set of caching strategies for big
               active data (BAD) systems. BAD is a data management paradigm
               that allows ingestion of massive amount of data from
               heterogeneous sources, such as sensor data, social networks, web
               and crowdsourced data in a large data cluster consisting of many
               computing and storage nodes, and enables a very large number of
               end users to subscribe to those data items through declarative
               subscriptions. A set of distributed broker nodes connect these
               end users to the backend data cluster, manage their
               subscriptions and deliver the subscription results to the end
               users. Unlike the most traditional publish-subscribe systems
               that match subscriptions against a single stream of publications
               to generate notifications, BAD can match subscriptions across
               multiple publications (by leveraging storage in the backend) and
               thus can enrich notifications with a rich set of diverse
               contents. As the matched results are delivered to the end users
               through the brokers, the broker node caches the results for a
               while so that the subscribers can retrieve them with reduced
               latency. Interesting research questions arise in this context so
               as to determine which result objects to cache or drop when the
               cache becomes full (eviction-based caching) or to admit objects
               with an explicit expiration time indicating how much time they
               should reside in the cache (TTL based caching). To this end, we
               propose a set of caching strategies for the brokers and show
               that the schemes achieve varying degree of efficiency in terms
               of notification delivery in the BAD system. We evaluate our
               schemes via a prototype implementation and through detailed
               simulation studies.",
  pages     = "696--705",
  month     =  jul,
  year      =  2018,
  keywords  = "Big Data;cache storage;pattern clustering;edge caching;caching
               strategies;big active data systems;data management paradigm;end
               users;data items;declarative subscriptions;distributed broker
               nodes;backend data cluster;eviction-based caching;TTL based
               caching;notification delivery;BAD system;notifications
               delivery;Big Data;Distributed databases;Publish-subscribe;Task
               analysis;Computer science;Electronic mail;Big active
               data;bigdata publish-subscribe system;caching policies;TTL
               caching;DSM"
}

@INPROCEEDINGS{2018-middleware-poster,
  title     = "An Implementation Experience with {SDN-enabled} {IoT} Data
               Exchange Middleware",
  booktitle = "Proceedings of the 19th International Middleware Conference
               (Posters)",
  author    = "Scalzotto, Luca and Benson, Kyle E and Bouloukakis, Georgios and
               Bellavista, Paolo and Issarny, Val{\'e}rie and Mehrotra, Sharad
               and Venkatasubramanian, Nalini",
  abstract  = "This poster presents our prototype implementation of FireDeX
               [1], a cross-layer middleware that supports timely delivery of
               mission-critical messages (i.e. events) over an IoT data
               exchange service. Emergency scenarios may challenge/congest the
               network infrastructure. FireDeX addresses these situations by
               prioritizing event delivery and by dropping some low priority
               events.",
  publisher = "Association for Computing Machinery",
  pages     = "21--22",
  series    = "Middleware '18",
  month     =  dec,
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "DSM",
  location  = "Rennes, France"
}

@INPROCEEDINGS{2018-firedex,
  title     = "{FireDeX}: a Prioritized {IoT} Data Exchange Middleware for
               Emergency Response",
  booktitle = "Proceedings of the 19th International Middleware Conference",
  author    = "Benson, Kyle E and Bouloukakis, Georgios and Grant, Casey and
               Issarny, Val{\'e}rie and Mehrotra, Sharad and Moscholios,
               Ioannis and Venkatasubramanian, Nalini",
  abstract  = "Real-time event detection and targeted decision making for
               emerging mission-critical applications, e.g. smart fire
               fighting, requires systems that extract and process relevant
               data from connected IoT devices in the environment. In this
               paper, we propose FireDeX, a cross-layer middleware that
               facilitates timely and effective exchange of data for
               coordinating emergency response activities. FireDeX adopts a
               publish-subscribe data exchange paradigm with brokers at the
               network edge to manage prioritized delivery of mission-critical
               data from IoT sources to relevant subscribers. It incorporates
               parameters at the application, network, and middleware layers
               into a data exchange service that accurately estimates
               end-to-end performance metrics (e.g. delays, success rates). We
               design an extensible queueing theoretic model that abstracts
               these cross-layer interactions as a network of queues, thereby
               making it amenable for rapid analysis. We propose novel
               algorithms that utilize results of this analysis to tune data
               exchange configurations (event priorities and dropping policies)
               while meeting situational awareness requirements and resource
               constraints. FireDeX leverages Software-Defined Networking (SDN)
               methodologies to enforce these configurations in the IoT network
               infrastructure. We evaluate its performance through simulated
               experiments in a smart building fire response scenario. Our
               results demonstrate significant improvement to mission-critical
               data delivery under a variety of conditions. Our
               application-aware prioritization algorithm improves the value of
               exchanged information by 36\% when compared with no
               prioritization; the addition of our network-aware drop rate
               policies improves this performance by 42\% over priorities only
               and by 94\% over no prioritization.",
  publisher = "Association for Computing Machinery",
  pages     = "279--292",
  series    = "Middleware '18",
  month     =  nov,
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "SDN, Utility Functions, Publish/Subscribe Middleware, Emergency
               Response, Event Prioritization, Queueing Networks;DSM",
  location  = "Rennes, France"
}

@INPROCEEDINGS{2018-srds,
  title     = "Enabling State Estimation for Fault Identification in Water
               Distribution Systems Under Large Disasters",
  booktitle = "2018 {IEEE} 37th Symposium on Reliable Distributed Systems
               ({SRDS})",
  author    = "Han, Q and Eguchi, R and Mehrotra, S and Venkatasubramanian, N",
  abstract  = "We present a graphical model based approach for on-line state
               estimation of water distribution system failures during
               large-scale disasters. Water distribution systems often exhibit
               extreme fragilities during large-scale disasters (e.g.,
               earthquakes) resulting in massive pipe breaks, water
               contamination, and disruption of service. To monitor and
               identify potential problems, hidden state information must be
               extracted from limited and noisy data environments. This
               requires estimating the operating states of the water system
               quickly and accurately. We model the water system as a factor
               graph, characterizing the non-linearity of fluid flow in a
               network that is dynamically altered by leaks, breaks and
               operations designed to minimize water loss. The approach
               considers a structured probabilistic framework which models
               complex interdependencies within a high-level network topology.
               The proposed two-phase approach, which begins with a network
               decomposition using articulation points followed by the
               distributed Gauss-Newton Belief Propagation (GN-BP) based
               inference, can deliver optimal estimates of the system state in
               near real-time. The approach is evaluated in canonical and
               real-world water systems under different levels of physical and
               cyber disruptions, using the Water Network Tool for Resilience
               (WNTR) recently developed by Sandia National Lab and
               Environmental Protection Agency (EPA). Our results demonstrate
               that the proposed GN-BP approach can yield an accurate
               estimation of system states (mean square error 0.02) in a
               relatively fast manner (within 1s). The two-phase mechanism
               enables the scalability of state estimation and provides a
               robust assessment of performance of large-scale water systems in
               terms of computational complexity and accuracy. A case study on
               the identification of ``faulty zones'' shows that 80\% broken
               pipelines and 99\% loss-of-service to end-users can be
               localized.",
  pages     = "161--170",
  month     =  oct,
  year      =  2018,
  keywords  = "backpropagation;computational complexity;disasters;fault
               diagnosis;Gaussian processes;graph theory;neural nets;Newton
               method;optimisation;pipelines;probability;state estimation;water
               supply;large-scale water systems;Water distribution
               systems;on-line state estimation;water distribution system
               failures;large-scale disasters;high-level network
               topology;distributed Gauss-Newton Belief Propagation;graphical
               model;water loss minimization;GN-BP;water network tool for
               resilience;WNTR;Sandia National Lab;Environmental Protection
               Agency;omputational complexity;Hydraulic systems;State
               estimation;Noise measurement;Graphical models;Probabilistic
               logic;Pipelines;Water pollution;cyber physical system;graphical
               model;belief propagation;failure detection;water system;DSM"
}

@PHDTHESIS{2018-kyle-thesis,
  title    = "Resilient Communications Middleware for {IoT} Data Exchange",
  author   = "Benson, Kyle Edward",
  abstract = "Author(s): Benson, Kyle Edward | Advisor(s): Venkatasubramanian,
              Nalini | Abstract: IoT aims to improve our daily lives by
              seamlessly integrating heterogeneous devices into the physical
              space around us through a digital ecosystem of sensor data,
              communications, analytics, and semi-automated actions. Our
              exploratory proof-of-concept IoT system, SCALE, motivated our
              research challenges and served as a testbed for deploying our
              proposed approach. In particular, we identified the critical need
              for IoT data exchange: an event-driven pattern using
              producer-consumer abstractions to connect many heterogeneous
              devices, services, and users. This provides a seamless
              communications fabric for large-scale networks of heterogeneous
              resource-constrained devices. IoT deployments keep system
              deployment costs and complexity low by off-loading much of the
              logic to the cloud. Hence, data exchange manages the flow of
              application-supporting messages in order to gather data at the
              network edge, process it in the cloud, and then use it at the
              edge. However, infrastructure failures (e.g. during a natural
              disaster) or resource limitations (e.g. during emergency response
              activities) disrupt connectivity with such cloud platforms.To
              ensure a high degree of confidence in the resilient operation of
              mission-critical IoT systems, this thesis proposes middleware
              solutions to resilient communications in support of IoT data
              exchange in highly-challenged environments. Our proposed
              techniques leverage current application information/resilience
              requirements, physical network topology awareness, and modern
              system configuration abstractions (i.e. edge computing and SDN).
              They account for and adapt the IoT data exchange to failures and
              other impactful constraints in the underlying network
              infrastructure. We propose the use of SDN-enabled edge computing
              for three primary reasons: reliability, performance, and locality
              (i.e. making use of data produced at the edge there at the edge).
              We explore this approach within the context of two different
              mission-critical scenarios and three projects that build on each
              other to progressively leverage more edge intelligence as we
              focus on more local settings. We first leverage a
              centrally-controlled geo-aware resilient overlay network
              (GeoCRON) to improve cloud-centric collection of IoT seismic
              sensor data during geographically-widespread earthquake-induced
              network infrastructure failures. We then consider the question of
              whether to collect and process IoT data in the cloud or at the
              edge for further resilience to cloud connection instability. To
              this end, the Ride system monitors cloud overlay paths and
              redirects IoT data flows (when necessary) transparently to IoT
              devices. Ride also expands on the seismic scenario with
              earthquake early-warning through a novel resilient local alerting
              mechanism based on redundant multicast trees. Lastly, we consider
              balancing the needs of multiple mission-critical applications in
              an IoT-enabled structure fire response. The FireDeX project
              models the complete data exchange (i.e. IoT hosts, data exchange
              brokers, and network infrastructure) and adaptively prioritizes
              information flows according to information requirements and
              network resource constraints.Altogether, the proposed middleware
              approach enables a more holistic view of and control over the
              data exchange process. As demonstrated in the individual
              projects, this occurs at different network and IoT deployment
              scales. While these contributions are but a few parts of the
              greater resilient IoT data exchange challenge, they represent a
              few steps in the direction of that goal.",
  year     =  2018,
  school   = "UC Irvine",
  keywords = "DSM"
}
